# Final Project

In this final project, you will convert a convert notebook style code for comparing different
tabular data prediction tasks into a shareable
BenchOpt benchmark --- you will then extend the benchmark with more methods, datasets, and 
control parameters.

## Prerequisites

This homeworks assumes you already possess a github account. It also assumes you have `git`, `pip`, and `conda` 
installed on your computer. If not, there are many resources online for installing them: Please 
independently install them using the resource of your choice before proceeding.

## Read Notebook

1. Go to [https://github.com/xyhan-github/STATS335_F23/blob/master/hw8/TabularDemo.ipynb](https://github.com/xyhan-github/STATS335_F23/blob/master/hw8/TabularDemo.ipynb).

    ####
    
    This notebook contains a simplified version of the code from hw7 that does the following:
    * Loads example tabular datasets from Huggingface:
      * Adult Income: Binary Classification
      * Housing Prices: Regression
      * Forest Covertype: Multi-Class Classification
    * Cleans the data and identifies the type of problem it is.
    * Trains a model Linear, XGBoost, or LightGBM on the data.
   
2. Click the "Open in Colab" button.

3. In Google Colab, read and run the code in the notebook to make sure you understand what it does.

## Setup BenchOpt

1. Open a terminal window on your computer.

3. Install BenchOpt on your computer by typing the following into terminal.
   ```
   $ pip install -U benchopt
   ```

2. Navigate to a directory where you want to store your BenchOpt benchmarks.
   ####
   (In the terminal, you can navigate to a directory by typing `cd <directory path>`.)

3. In terminal, clone the BenchOpt benchmark template into the folder and navigate into it.
   ```
   $ git clone https://github.com/xyhan-github/stats-335_tabular_data
   $ cd stats-335_tabular_data
   ```
   
4. In terminal, install the benchopt dependencies by running
    ```
    $ benchopt install
    ```

5. In terminal, run the template benchmark by running
    ```
    $ benchopt run
    ```
    If everything was set up correctly, this should generate a .html file of results
    that contains the results of running the housing prices and adult income datasets
    on the linear classification model (regression or logistic regression depending on the dataset)
    across three parameter setting for the $L_2$ regularization parameter (`reg_lambda`).
   
## Task 1: Finish adapting the notebook

1. Study the `objective.py`, `solvers/linear_l2.py`, and `datasets/datasets_huggingface.py` files.
2. Using the BenchOpt documentation

   [https://benchopt.github.io/tutorials/build_benchmark.html](https://benchopt.github.io/tutorials/build_benchmark.html)

   get a sense for how the benchmark was adapted from the parts of notebook you read earlier.

3. The notebook is only partially adapted: The benchmark is missing the following:
   * The Forest Covertype dataset
   * The XGBoost and LightGBM solvers
   
   Add these to the benchmark by using the notebook and existing files as a guide.

4. **Submit:** Perform `benchopt run` again. Submit the resulting .html file to Canvas.

# Task 2: Extend the benchmark

1. Extend the benchmark by doing the following:
   * Add three more datasets of your choice to the benchmark by either extending the
     `datasets/datasets_huggingface.py` file or by writing your own datasets file.
   * Add two more solvers of your choice to the benchmark by writing your own solver files.
2. Expand the solvers (both the ones you added and the original Linear, XGBoost, and LightGBM solvers),
   such that each varies least three different control parameters (such as `reg_lambda`, `max_depth`, etc).
   * Overall, there should be at least 8 different experiments performed for each dataset-method combination.
3. Run the extended benchmark using `benchopt run`.
4. Follow the instructions in the below documentation to publish your results to BenchOpt:
   [https://benchopt.github.io/benchmark_workflow/publish_benchmark.html?highlight=publish](https://benchopt.github.io/benchmark_workflow/publish_benchmark.html?highlight=publish)

5. **Submit:** Submit the following:
   * The HTML file generated by your BenchOpt run of the extended benchmark.
   * The URL of the pull-requested created when you called `benchopt publish`.
     * It should look something like `https://github.com/benchopt/results/pull/<some number>`.

## Submitting on Canvas

On canvas, submit the following:

1. The requested HTML files and URL from Tasks 1 and 2.
2. **Short Answer Questions:**
   In a PDF document, answer the following short-answer questions:
   * Compare BenchOpt to other tools for MLops like Huggingface, Papers with Code, and Weights and Biases, 
     how is it similar? How is it different?
   * What do you think are the fields and tasks that would benefit from the usage of BenchOpt?
   * What do you think are the biggest obstacles that may prevent the widespread adoption of BenchOpt? Why?
3. **Long Answer Question**
   In the same PDF, write a 1-2 page proposal for a hypothetical challenge. Assume, say, you have $1M of prize money
   to work with. Describe the following:
   * What would be the task of the challenge and the problem it seems to address.
   * What will be the `rules` of the challenge.
   * What tools will could be used to host, evaluate, and share the results of the challenge.
   * How would you allocate the prize money?
   * How does your proposed challenge compare to some of those we studied in class?